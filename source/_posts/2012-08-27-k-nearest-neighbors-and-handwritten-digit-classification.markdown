---
layout: post
title: "Метод K-ближайших соседей и распознавание рукописных цифр"
date: 2012-08-27 21:09
comments: true
categories: [Russian, translations, meachine learning, math, python]
---

Источник [K-Nearest-Neighbors and Handwritten Digit Classification](http://jeremykun.wordpress.com/2012/08/26/k-nearest-neighbors-and-handwritten-digit-classification/).

## Рецепт классификации

Одна из важных задач машинного обучения (далее МО) - это распределение объектов по нескольких известных классов. Например, Вам нужно отличить полезное письмо от спама. Или Вы хотите определить вид жука на основе его физических параметров, таких как: вес, цвет, длина челюсти. Эти "атрибуты", часто называемые _свойствами_ в терминах МО, и так же часто интерпретируются, как _размерностью_ в рамках линейной алгебры. В качестве разминка для читателей: "Какие __свойства__ имеет электронное письмо?". Верных ответов будет достаточно много.

<!-- MORE -->

Общепринятый метод классификации данных является обучение с _учителем_. Это когда мы предоставляем набор уже классифицированных данных на вход алгоритма обучения, который создает внутреннюю модель задачи и используем отдельный алгоритм классификации, который в свою очередь использует эту внутреннюю модель для классификации новых данных. Фаза тренировки обычно сложная, а алгоритм классификации прост, хотя это может быть и не так для методов, которые мы будем исследовать в этом посте.

Чаще всего, входные данные для алгоритма обучения преобразуются одним из подходящим способов в числовое представление. Это не так просто, как звучит. Для этого мы отделим данные от области приложения, что в некоторой степени позволит применять математический анализ. Мы можем сфокусировать наши вопросы на _данные_, а не на _задачи_. В общем, это основной принцип применения математики: извлечь из задачи сущность вопроса, на который мы хотим ответить, в чистом математическом виде, после интерпретируем результат. 

В наше статье о [персептроне](http://jeremykun.wordpress.com/2011/08/11/the-perceptron-and-all-the-things-it-cant-perceive/), мы вывели алгоритм для поиска линии, которая разделяет все точки одного класса от другого, априори предполагая, что она существует. В данной посте, однако, мы делаем другие структурные предположения. Именно, мы допускаем, что данные - точки, которые находятся в одном классе и также расположены близко друг к другу, соотносятся в **подходящей метрике**. Читатель может отметить следующую нестандартную терминологию, это просто математическое переопределение того, что мы уже сказали.

**Аксиома соседства**: Пусть $$(X, d)$$ будет [метрическим пространством](http://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D1%80%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%82%D0%B2%D0%BE) и пусть $$ S \in X $$ будет конечным множеством, элементы которого классифицируются через функцию $$ f:S \rightarrow \{ 1,2,..m \} $$. Мы говорим, что $$ S $$ удовлетворяет _аксиоме соседства_, если для каждой точки $$ x \in S $$ есть $$ y $$ ближайшая к $$ x $$, тогда $$ f(x) = f(y) $$. То есть $$ y $$ разделяет тот же класс, что и $$ x $$, если $$ y $$ ближайший сосед $$ x $$.

Для более глубокого понимания метрики, читатель может посмотреть [пример](http://jeremykun.wordpress.com/2012/08/26/metric-spaces-a-primer/). Для нашего поста и всех будущих, $$ X $$ будет всегда $$ \mathbb{R}^n $$ для любого $$ n $$, в то время как метрика $$ d $$ будет меняться.

Эта аксиома действительно очень _смелое_ предположение, которое определенно не истинно для множества наборов данных. Практически, это сильно зависит от постановки задачи. Наличие неправильных типов или количества свойств, неправильные преобразования, или использование неправильных метрик может сделать предположение некорректным, даже если задача от природы имеет нужную структуру. К счастью, для реальных приложений нам нужно только прилепить данные к аксиоме соседства приближенно (конечно, аксиома только проверяется в приближении). Также мы подразумеваем под "приближением" также зависимость от задачи и терпимость пользователя к ошибкам. Такова природа прикладной математики.

Как только мы поймем аксиому, алгоритм МО по существу станет очевидным. Для разминки: Есть большое количество точек, чьи классы известны и фиксированы на метрическом пространстве. Чтобы определить класс неизвестной точки, достаточно использовать самый общий класс ее ближайших соседей. Т.к. количество соседей может меняться, этот метод интуитивно называется *метод К-ближайших соседей (КБС)*.

## Самый основной способ обучения: копирование соседей.

Давайте перейдем к деталям программы и для теста создадим "болванку" данных. Это будет набор точек из $$ \mathbb {R}^2 $$, которые явно удовлетворяют условиям аксиомы соседства. Сделаем это с помощью Питона, создав набор данных из 2-х независимых нормально распределенных массивов случайных чисел:

{% codeblock lang:python %}
import random

def gaussCluster(center, stdDev, count=50):
    return [(random.gauss(center[0], stdDev),
             random.gauss(center[1], stdDev)) for _ in range(count)]

def makeDummyData():
    return gaussCluster((-4,0), 1) + gaussCluster((4,0), 1)
{% endcodeblock %}

Первая функция просто возвращает набор точек, которые определенны нормальным распределением. Для простоты мы сделали ковариацию двух случайных величин одинаковой. Вторя функция собирает два набора в один массив. 

Теперь дадим классам болванки "метки", мы будем просто иметь второй список, чтобы хранить рядом с данными. Индекс точки в первом массиве будет соответствовать индексу его метки во втором. Есть и более элегантные способы организации данных, но сейчас сойдет и так. 

Реализация метрики так же проста. Сейчас, мы будем использовать стандартную Эвклидову метрику. Т.е. мы просто берем сумму квадратов координат двух точек.


{% codeblock lang:python %}
import math

def euclideanDistance(x,y):
    return math.sqrt(sum([(a-b)**2 for (a,b) in zip(x,y)]))
{% endcodeblock %}

Для реализации классификатора, мы создадим функцию, которая сама вернет нам функцию.


{% codeblock lang:python %}
import heapq

def makeKNNClassifier(data, labels, k, distance):
    def classify(x):
        closestPoints = heapq.nsmallest(k, enumerate(data),
                                        key=lambda y: distance(x, y[1]))м
        closestLabels = [labels[i] for (i, pt) in closestPoints]
        return max(set(closestLabels), key=closestLabels.count)

    return classify
{% endcodeblock %}

Есть несколько хитрых вещей в этой функции, которые заслуживают обсуждения. Первое и главное, мы определяем функцию внутри другой функции, и возвращаем созданную функцию. Важным техническим моментом здесь является то, что созданная функция сохранит все локальные переменные, которые есть в ее области даже если функция завершиться. Попробуйте, вы сможете вызвать _makeKNNClassifier_ несколько раз с разными аргументами , и возвращаемые функции не будут мешать друг другу. Это особенность языка программирования, называется _замыкание_ (*closure*). Это позволяет нам, например, сохранять важные данные видимыми и в тоже время скрывать данные нижнего уровня, зависимые, но которые не могут быть доступны напрямую. 

Второй момент, мы используем некоторые Питоновские конструкции. Первая линия классификатора использует *heapq* для хранения $$ k $$ наименьших элементов массива данных, но в добавок мы используем *enumerate* для хранения индекса возвращаемого элемента, и ключ по которому мы будем определять "ближайшее" определяется с помощью функции, переданной в качестве аргумента. Заметим, что индекс _y[1]_ в лямбда функции используется, как "y" координату и не сохраняет индекс.

Вторая линия просто извлекает список меток соответствующий каждой из ближайших точек, которые мы получили после вызова "nsmallest". В заключение, третья линия возвращает максимум из данных меток, где вес метки (определенный через плохо названную _key_ lambda) это их количество в _closestLabels_ списке.

Используем этих в функций в небольшом примере:

{% codeblock lang:python %}
trainingPoints = makeDummyData() # has 50 points from each class
trainingLabels = [1] * 50 + [2] * 50  # an arbitrary choice of labeling

f = makeKNNClassifier(trainingPoints, trainingLabels, 8, euclideanDistance)
print f((-3,0))
{% endcodeblock %}

Читатель может повозиться с примером как хочет, но мы не будем заниматься этим дальше. Как обычно, весь код в этом посте доступен [здесь](http://code.google.com/p/math-intersect-programming/downloads/list). Перейдем к вещам более сложным.

## Рукописные цифры.

Есть почти классический пример в литературе по классификации - это распознавание рукописных цифр. Эта задача оригинально появилась (как гласит легенда) в Почтовой Службе США для цели автоматизации сортировки почты по почтовым индексам. ... Давайте посмотрим, как наш алгоритм воплотит это в реальность.

Мы возьмем данные из [UCI](http://archive.ics.uci.edu/ml/) репозитория с небольшими изменениями, мы предоставляем из [здесь](http://code.google.com/p/math-intersect-programming/downloads/list). Одна линия файла данный предоставляет рукописную цифру и ее метку. Цифра это вектор размерностью 256 полученный путем упаковки 16х16 двоичной картинки в ряд по строкам. Меткой будет число, которое представлено на картинке. Наш файл содержит 1593 примера примерно по 160 на цифру.

Другими словами, наше метрическое пространство $$ \{ 0,1 \}^{256} $$, и мы выбрали Эвклидово пространство для простоты. Линия наших данных выглядит вот так:

{% codeblock %}

0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 
0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 
0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 
0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 
0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 
0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 
0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 
0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 
1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 
1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 
1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 
1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 
1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 
1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 
1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 
0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0, 6

{% endcodeblock %}

После чтения данных, мы случайным образом делим данный на 2 массива, первый будет для обучения, второй для проверки. Следующая функция делает это, возвращая оценку успешности алгоритма классификации на массиве проверочных данных. 

{% codeblock lang:python %}

mport knn
import random

def column(A, j):
   return [row[j] for row in A]

def test(data, k):
   random.shuffle(data)
   pts, labels = column(data, 0), column(data, 1)

   trainingData = pts[:800]
   trainingLabels = labels[:800]
   testData = pts[800:]
   testLabels = labels[800:]

   f = knn.makeKNNClassifier(trainingData, trainingLabels,
                             k, knn.euclideanDistance)
   correct = 0
   total = len(testLabels)

   for (point, label) in zip(testData, testLabels):
      if f(point) == label:
         correct += 1

   return float(correct) / total

{% endcodeblock %}

Запустив алгоритм с $$ k=1 $$ получаем 89% успешного выполнения. Меняя $$ k $$ мы видим изменение эффективности, без модификации алгоритма или метрики. В итоге, график ниже показывает, что рукописные данные вполне согласуются с аксиомой ближайших соседей.

![]( {{ root_url }}/images/posts/k-vs-percentage-correct.png  "A graph of classification accuracy against k for values of k between 1 and 50. The graph clearly shows a downward trend as k increases, but all values k < 10 are comparably go")

Также возможно улучшить эту программу и мы могли бы сделать более подходящий алгоритм. Но рассматривая, что алгоритм использует необработанные данные и не манипулирует с с ними, результат не так уж плох.

В качестве примечания, могло бы быть гораздо интереснее, взять какое-нибудь планшетное программное обеспечение и использовать данный метод для распознавания цифр написанных на нем. Но увы, у нас мало времени для такого рода приложений.

## Преимущество, улучшения и проблемы.

Первое преимущество _k-ближайших соседей_ - общий и широко-известный алгоритм, который легко реализуется. Конечно, мы сделали ядро алгоритма всего в три строчки. Но данный алгоритм легко распараллелить и он от природы гибок. В отличии от [персептрона][1], который полагается на линейное разделение, метод соседей и аксиома соседства доступны для данных, которые представлены в виде разных геометрических фигур. В этих [лекциях][2] есть хорошие примеры, как показано ниже, и читатель может конечно сделать еще больше фокусов. 

![][3]

Конечно, гибкость еще больше, в силу того, что можно использовать любые метрики для подсчета дистанций. Можно для примера, использовать [метрику Манхэттена][4], если точки располагаются в городе. Возможности ограничиваются только нахождением новых и полезных метрик. 

С такой популярностью KБС часто используется с некоторыми модификациями и улучшениями. Одно из улучшений - это гистерезис, который удаляет определенные точки рядом с границей решения. Такая техника называется модифицированный алгоритм КБС. Другое улучшение - это утяжелять определенные свойства при вычислении дистанции, которые требуют программного определения, какое свойство меньше полезно при классификации. Это уже ближе к области деревьев решений, поэтому мы оставим все это для тренировки читателя.

Следующее улучшение должно оптимизировать время выполнения. Если дано $$ n $$ учебных точек и $$ d $$ свойств, одна точка требует $$ O(nd) $$ для классификации. Это особенно расточительно, потому что большинство вычислений дистанций исполняются между точками, которые очень сильно рассеяны, и как $$ k $$ обычно мал, они не влияют на классификацию.

Один способ облегчить - это хранить точки в структуре, так называемых [k-деревья][5]. Данный метод возник в вычислительной геометрии в задаче о положении точки. Он разделяет пространство на части, основанные на количестве точек в каждом участке, и организовывает их в дерево. Другими словами, он может выделить узкую область, где точки расположены плотно, и грубо, где их меньше. Каждый шаг обхода дерева, может проверить, в какой суб-области лежат неклассифицириумые  точки, и решить надлежащим образом. С определенной гарантией, это уменьшит вычисления до $$ O(log(n)d) $$. К несчастью, есть проблемы в случае большого количества измерений, что выходит за рамки этого поста. Мы планируем исследовать k-d деревья в будущей серии о вычислительной геометрии.

Последняя проблема, которую мы рассмотрим - масштабирование данных. А именно, нужно быть осторожным, когда конвертируем реальные данные в числа. Мы можем думать о каждом свойстве, как о случайной переменной, и мы хотим, что бы все эти переменные имели сопоставимые изменения. Причина проста - мы используем _сферы_. Можно описать КБС, как поиск наименьшей сферы с центром в безымянной точке, которая содержит $$ k $$  именных точек данных, и использовать большее общее этих точек, что бы классифицировать новую. Конечно, можно говорить "сфера" в любом метрическом пространстве; это просто множество точек на фиксированном расстоянии от центра. Важное замечание то, что сфера имеет одинаковую длину для все осей. Если данные масштабируются не правильно, когда геометрия сферы не отражает геометрию данных, и алгоритм будет спотыкаться. 



[1]: http://jeremykun.wordpress.com/2011/08/11/the-perceptron-and-all-the-things-it-cant-perceive/
[2]: http://courses.cs.tamu.edu/rgutier/cs790_w02/l8.pdf
[3]: {{ root_url }}/images/posts/concentric-circles-knn.png "k-nearest-neighbors applied to a data set organized in concentric circles."
[4]: http://www.improvedoutcomes.com/docs/WebSiteDocs/Clustering/Clustering_Parameters/Manhattan_Distance_Metric.htm
[5]: http://ru.wikipedia.org/wiki/K-%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D0%B5_%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE
