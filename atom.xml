<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Flipback's blog]]></title>
  <link href="http://flipback.github.com/atom.xml" rel="self"/>
  <link href="http://flipback.github.com/"/>
  <updated>2015-02-01T21:43:34+05:00</updated>
  <id>http://flipback.github.com/</id>
  <author>
    <name><![CDATA[Aleksey Timin]]></name>
    <email><![CDATA[atimin@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[NMatrix 0.2.0 and Slicing]]></title>
    <link href="http://flipback.github.com/blog/2012/09/30/nmatrix-0-dot-2-0-and-slicing/"/>
    <updated>2012-09-30T19:43:00+06:00</updated>
    <id>http://flipback.github.com/blog/2012/09/30/nmatrix-0-dot-2-0-and-slicing</id>
    <content type="html"><![CDATA[<p>A few days ago the <a href="http://sciruby.com/blog/2012/09/24/second-nmatrix-alpha-released/">new version</a> of NMatrix has been released. Besides a lot new features it has slicing operations now. The slicing has been implemented by two ways. First of they it&rsquo;s a slicing by reference. It is provided by <strong>#[]</strong> method:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="nb">require</span> <span class="s2">&quot;nmatrix&quot;</span>
</span><span class='line'><span class="nb">require</span> <span class="s2">&quot;pp&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="c1"># Create dense matrix with size [3,3] and fill it.</span>
</span><span class='line'><span class="n">m</span> <span class="o">=</span> <span class="no">NMatrix</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="ss">:dense</span><span class="p">,</span> <span class="o">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="o">]</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="o">.</span><span class="n">.</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">to_a</span><span class="p">)</span>
</span><span class='line'><span class="n">r</span> <span class="o">=</span> <span class="n">m</span><span class="o">[</span><span class="mi">0</span><span class="o">.</span><span class="n">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">.</span><span class="n">.</span><span class="mi">2</span><span class="o">]</span> <span class="c1">#=&gt; [1,2] [4,5]</span>
</span><span class='line'><span class="n">r</span><span class="o">.</span><span class="n">is_ref?</span>         <span class="c1">#=&gt; true</span>
</span><span class='line'>
</span><span class='line'><span class="n">r</span><span class="o">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="o">]</span> <span class="o">=</span> <span class="mi">999</span>
</span><span class='line'>
</span><span class='line'><span class="n">pp</span> <span class="n">r</span>              <span class="c1">#=&gt; [999,2] [4,5]</span>
</span><span class='line'><span class="n">pp</span> <span class="n">m</span>              <span class="c1">#=&gt; [0,999,2] [3,4,5] [6,7,8]</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you see both matrices has been changed when we assign a new value to element of <em>r</em> matrix. The <strong>#[]</strong> method doesn&rsquo;t allocate memory,  but creates a new matrix with reference to a base matrix <em>m</em>. You can build chain of the same slicing and all of new matrices will be references for one base.</p>

<!-- MORE -->


<p></p>

<p><strong>Note:</strong> The NMatrix uses ATLAS for multiplication, which surely cannot work with references. And so when you try to multiply references they will be copped to temporary matrices before multiplication. Remember it using a big data. Also it is for several operations such as a casting.</p>

<p>Second method of slicing by copying is <strong>#slice</strong>. This method allocates memory for the slice and copies all elements from the base matrix. Example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="nb">require</span> <span class="s2">&quot;nmatrix&quot;</span>
</span><span class='line'><span class="nb">require</span> <span class="s2">&quot;pp&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="c1"># Create dense matrix with size [3,3] and fill it.</span>
</span><span class='line'><span class="n">m</span> <span class="o">=</span> <span class="no">NMatrix</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="ss">:dense</span><span class="p">,</span> <span class="o">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="o">]</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="o">.</span><span class="n">.</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">to_a</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">r</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="mi">0</span><span class="o">.</span><span class="n">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">.</span><span class="n">.</span><span class="mi">2</span><span class="p">)</span> <span class="c1">#=&gt; [1,2] [4,5]</span>
</span><span class='line'><span class="n">r</span><span class="o">.</span><span class="n">is_ref?</span>         <span class="c1">#=&gt; false</span>
</span><span class='line'>
</span><span class='line'><span class="n">r</span><span class="o">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="o">]</span> <span class="o">=</span> <span class="mi">999</span>
</span><span class='line'>
</span><span class='line'><span class="n">pp</span> <span class="n">r</span>              <span class="c1">#=&gt; [999,2] [4,5]</span>
</span><span class='line'><span class="n">pp</span> <span class="n">m</span>              <span class="c1">#=&gt; [0,1,2] [3,4,5] [6,7,8]</span>
</span></code></pre></td></tr></table></div></figure>


<p>This time the <em>r</em> matrix is an independent object. All changes of it doesn&rsquo;t concern the base matrix and you can use it as an usual matrix.</p>

<p>I ought to remind that the slicing operations are pretty raw. Will be careful to use it. If you have a bug, feel free to report about it <a href="https://github.com/SciRuby/nmatrix/issues?direction=desc&amp;milestone=2&amp;sort=created&amp;state=open">here</a>.</p>

<p>Thanks, Aleksey.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Метод K-ближайших соседей и распознавание рукописных цифр]]></title>
    <link href="http://flipback.github.com/blog/2012/08/27/k-nearest-neighbors-and-handwritten-digit-classification/"/>
    <updated>2012-08-27T21:09:00+06:00</updated>
    <id>http://flipback.github.com/blog/2012/08/27/k-nearest-neighbors-and-handwritten-digit-classification</id>
    <content type="html"><![CDATA[<p>Источник <a href="http://jeremykun.wordpress.com/2012/08/26/k-nearest-neighbors-and-handwritten-digit-classification/">K-Nearest-Neighbors and Handwritten Digit Classification</a>.</p>

<h2>Рецепт классификации</h2>

<p>Одна из важных задач машинного обучения (далее МО) - это распределение объектов по нескольких известных классов. Например, Вам нужно отличить полезное письмо от спама. Или Вы хотите определить вид жука на основе его физических параметров, таких как: вес, цвет, длина челюсти. Эти &ldquo;атрибуты&rdquo;, часто называемые <em>свойствами</em> в терминах МО, и так же часто интерпретируются, как <em>размерностью</em> в рамках линейной алгебры. В качестве разминка для читателей: &ldquo;Какие <strong>свойства</strong> имеет электронное письмо?&rdquo;. Верных ответов будет достаточно много.</p>

<!-- MORE -->


<p>Общепринятый метод классификации данных является обучение с <em>учителем</em>. Это когда мы предоставляем набор уже классифицированных данных на вход алгоритма обучения, который создает внутреннюю модель задачи и используем отдельный алгоритм классификации, который в свою очередь использует эту внутреннюю модель для классификации новых данных. Фаза тренировки обычно сложная, а алгоритм классификации прост, хотя это может быть и не так для методов, которые мы будем исследовать в этом посте.</p>

<p>Чаще всего, входные данные для алгоритма обучения преобразуются одним из подходящим способов в числовое представление. Это не так просто, как звучит. Для этого мы отделим данные от области приложения, что в некоторой степени позволит применять математический анализ. Мы можем сфокусировать наши вопросы на <em>данные</em>, а не на <em>задачи</em>. В общем, это основной принцип применения математики: извлечь из задачи сущность вопроса, на который мы хотим ответить, в чистом математическом виде, после интерпретируем результат.</p>

<p>В наше статье о <a href="http://jeremykun.wordpress.com/2011/08/11/the-perceptron-and-all-the-things-it-cant-perceive/">персептроне</a>, мы вывели алгоритм для поиска линии, которая разделяет все точки одного класса от другого, априори предполагая, что она существует. В данной посте, однако, мы делаем другие структурные предположения. Именно, мы допускаем, что данные - точки, которые находятся в одном классе и также расположены близко друг к другу, соотносятся в <strong>подходящей метрике</strong>. Читатель может отметить следующую нестандартную терминологию, это просто математическое переопределение того, что мы уже сказали.</p>

<p><strong>Аксиома соседства</strong>: Пусть $(X, d)$ будет <a href="http://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D1%80%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%82%D0%B2%D0%BE">метрическим пространством</a> и пусть $ S \in X $ будет конечным множеством, элементы которого классифицируются через функцию $ f:S \rightarrow { 1,2,..m } $. Мы говорим, что $ S $ удовлетворяет <em>аксиоме соседства</em>, если для каждой точки $ x \in S $ есть $ y $ ближайшая к $ x $, тогда $ f(x) = f(y) $. То есть $ y $ разделяет тот же класс, что и $ x $, если $ y $ ближайший сосед $ x $.</p>

<p>Для более глубокого понимания метрики, читатель может посмотреть <a href="http://jeremykun.wordpress.com/2012/08/26/metric-spaces-a-primer/">пример</a>. Для нашего поста и всех будущих, $ X $ будет всегда $ \mathbb{R}^n $ для любого $ n $, в то время как метрика $ d $ будет меняться.</p>

<p>Эта аксиома действительно очень <em>смелое</em> предположение, которое определенно не истинно для множества наборов данных. Практически, это сильно зависит от постановки задачи. Наличие неправильных типов или количества свойств, неправильные преобразования, или использование неправильных метрик может сделать предположение некорректным, даже если задача от природы имеет нужную структуру. К счастью, для реальных приложений нам нужно только прилепить данные к аксиоме соседства приближенно (конечно, аксиома только проверяется в приближении). Также мы подразумеваем под &ldquo;приближением&rdquo; также зависимость от задачи и терпимость пользователя к ошибкам. Такова природа прикладной математики.</p>

<p>Как только мы поймем аксиому, алгоритм МО по существу станет очевидным. Для разминки: Есть большое количество точек, чьи классы известны и фиксированы на метрическом пространстве. Чтобы определить класс неизвестной точки, достаточно использовать самый общий класс ее ближайших соседей. Т.к. количество соседей может меняться, этот метод интуитивно называется <em>метод К-ближайших соседей (КБС)</em>.</p>

<h2>Самый основной способ обучения: копирование соседей.</h2>

<p>Давайте перейдем к деталям программы и для теста создадим &ldquo;болванку&rdquo; данных. Это будет набор точек из $ \mathbb {R}^2 $, которые явно удовлетворяют условиям аксиомы соседства. Сделаем это с помощью Питона, создав набор данных из 2-х независимых нормально распределенных массивов случайных чисел:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">random</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">gaussCluster</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="n">stdDev</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="p">[(</span><span class="n">random</span><span class="o">.</span><span class="n">gauss</span><span class="p">(</span><span class="n">center</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">stdDev</span><span class="p">),</span>
</span><span class='line'>             <span class="n">random</span><span class="o">.</span><span class="n">gauss</span><span class="p">(</span><span class="n">center</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">stdDev</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">)]</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">makeDummyData</span><span class="p">():</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">gaussCluster</span><span class="p">((</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">gaussCluster</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Первая функция просто возвращает набор точек, которые определенны нормальным распределением. Для простоты мы сделали ковариацию двух случайных величин одинаковой. Вторя функция собирает два набора в один массив.</p>

<p>Теперь дадим классам болванки &ldquo;метки&rdquo;, мы будем просто иметь второй список, чтобы хранить рядом с данными. Индекс точки в первом массиве будет соответствовать индексу его метки во втором. Есть и более элегантные способы организации данных, но сейчас сойдет и так.</p>

<p>Реализация метрики так же проста. Сейчас, мы будем использовать стандартную Эвклидову метрику. Т.е. мы просто берем сумму квадратов координат двух точек.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">math</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">euclideanDistance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">([(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)]))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Для реализации классификатора, мы создадим функцию, которая сама вернет нам функцию.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">heapq</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">makeKNNClassifier</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">distance</span><span class="p">):</span>
</span><span class='line'>    <span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span><span class='line'>        <span class="n">closestPoints</span> <span class="o">=</span> <span class="n">heapq</span><span class="o">.</span><span class="n">nsmallest</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">),</span>
</span><span class='line'>                                        <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span><span class="err">м</span>
</span><span class='line'>        <span class="n">closestLabels</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">pt</span><span class="p">)</span> <span class="ow">in</span> <span class="n">closestPoints</span><span class="p">]</span>
</span><span class='line'>        <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">closestLabels</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="n">closestLabels</span><span class="o">.</span><span class="n">count</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">return</span> <span class="n">classify</span>
</span></code></pre></td></tr></table></div></figure>


<p>Есть несколько хитрых вещей в этой функции, которые заслуживают обсуждения. Первое и главное, мы определяем функцию внутри другой функции, и возвращаем созданную функцию. Важным техническим моментом здесь является то, что созданная функция сохранит все локальные переменные, которые есть в ее области даже если функция завершиться. Попробуйте, вы сможете вызвать <em>makeKNNClassifier</em> несколько раз с разными аргументами , и возвращаемые функции не будут мешать друг другу. Это особенность языка программирования, называется <em>замыкание</em> (<em>closure</em>). Это позволяет нам, например, сохранять важные данные видимыми и в тоже время скрывать данные нижнего уровня, зависимые, но которые не могут быть доступны напрямую.</p>

<p>Второй момент, мы используем некоторые Питоновские конструкции. Первая линия классификатора использует <em>heapq</em> для хранения $ k $ наименьших элементов массива данных, но в добавок мы используем <em>enumerate</em> для хранения индекса возвращаемого элемента, и ключ по которому мы будем определять &ldquo;ближайшее&rdquo; определяется с помощью функции, переданной в качестве аргумента. Заметим, что индекс <em>y<a href="http://jeremykun.wordpress.com/2011/08/11/the-perceptron-and-all-the-things-it-cant-perceive/">1</a></em> в лямбда функции используется, как &ldquo;y&rdquo; координату и не сохраняет индекс.</p>

<p>Вторая линия просто извлекает список меток соответствующий каждой из ближайших точек, которые мы получили после вызова &ldquo;nsmallest&rdquo;. В заключение, третья линия возвращает максимум из данных меток, где вес метки (определенный через плохо названную <em>key</em> lambda) это их количество в <em>closestLabels</em> списке.</p>

<p>Используем этих в функций в небольшом примере:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">trainingPoints</span> <span class="o">=</span> <span class="n">makeDummyData</span><span class="p">()</span> <span class="c"># has 50 points from each class</span>
</span><span class='line'><span class="n">trainingLabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">50</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">50</span>  <span class="c"># an arbitrary choice of labeling</span>
</span><span class='line'>
</span><span class='line'><span class="n">f</span> <span class="o">=</span> <span class="n">makeKNNClassifier</span><span class="p">(</span><span class="n">trainingPoints</span><span class="p">,</span> <span class="n">trainingLabels</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">euclideanDistance</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="n">f</span><span class="p">((</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Читатель может повозиться с примером как хочет, но мы не будем заниматься этим дальше. Как обычно, весь код в этом посте доступен <a href="http://code.google.com/p/math-intersect-programming/downloads/list">здесь</a>. Перейдем к вещам более сложным.</p>

<h2>Рукописные цифры.</h2>

<p>Есть почти классический пример в литературе по классификации - это распознавание рукописных цифр. Эта задача оригинально появилась (как гласит легенда) в Почтовой Службе США для цели автоматизации сортировки почты по почтовым индексам. &hellip; Давайте посмотрим, как наш алгоритм воплотит это в реальность.</p>

<p>Мы возьмем данные из <a href="http://archive.ics.uci.edu/ml/">UCI</a> репозитория с небольшими изменениями, мы предоставляем из <a href="http://code.google.com/p/math-intersect-programming/downloads/list">здесь</a>. Одна линия файла данный предоставляет рукописную цифру и ее метку. Цифра это вектор размерностью 256 полученный путем упаковки 16х16 двоичной картинки в ряд по строкам. Меткой будет число, которое представлено на картинке. Наш файл содержит 1593 примера примерно по 160 на цифру.</p>

<p>Другими словами, наше метрическое пространство $ { 0,1 }^{256} $, и мы выбрали Эвклидово пространство для простоты. Линия наших данных выглядит вот так:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 
</span><span class='line'>0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 
</span><span class='line'>0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 
</span><span class='line'>0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 
</span><span class='line'>0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 
</span><span class='line'>0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 
</span><span class='line'>0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 
</span><span class='line'>0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 
</span><span class='line'>1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 
</span><span class='line'>1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 
</span><span class='line'>1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 
</span><span class='line'>1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 
</span><span class='line'>1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 
</span><span class='line'>1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 
</span><span class='line'>1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 
</span><span class='line'>0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0, 6</span></code></pre></td></tr></table></div></figure>


<p>После чтения данных, мы случайным образом делим данный на 2 массива, первый будет для обучения, второй для проверки. Следующая функция делает это, возвращая оценку успешности алгоритма классификации на массиве проверочных данных.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">mport</span> <span class="n">knn</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">random</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">column</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
</span><span class='line'>   <span class="k">return</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">A</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span><span class='line'>   <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span><span class='line'>   <span class="n">pts</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">column</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">column</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>   <span class="n">trainingData</span> <span class="o">=</span> <span class="n">pts</span><span class="p">[:</span><span class="mi">800</span><span class="p">]</span>
</span><span class='line'>   <span class="n">trainingLabels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:</span><span class="mi">800</span><span class="p">]</span>
</span><span class='line'>   <span class="n">testData</span> <span class="o">=</span> <span class="n">pts</span><span class="p">[</span><span class="mi">800</span><span class="p">:]</span>
</span><span class='line'>   <span class="n">testLabels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="mi">800</span><span class="p">:]</span>
</span><span class='line'>
</span><span class='line'>   <span class="n">f</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">makeKNNClassifier</span><span class="p">(</span><span class="n">trainingData</span><span class="p">,</span> <span class="n">trainingLabels</span><span class="p">,</span>
</span><span class='line'>                             <span class="n">k</span><span class="p">,</span> <span class="n">knn</span><span class="o">.</span><span class="n">euclideanDistance</span><span class="p">)</span>
</span><span class='line'>   <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'>   <span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">testLabels</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>   <span class="k">for</span> <span class="p">(</span><span class="n">point</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">testData</span><span class="p">,</span> <span class="n">testLabels</span><span class="p">):</span>
</span><span class='line'>      <span class="k">if</span> <span class="n">f</span><span class="p">(</span><span class="n">point</span><span class="p">)</span> <span class="o">==</span> <span class="n">label</span><span class="p">:</span>
</span><span class='line'>         <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class='line'>
</span><span class='line'>   <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">total</span>
</span></code></pre></td></tr></table></div></figure>


<p>Запустив алгоритм с $ k=1 $ получаем 89% успешного выполнения. Меняя $ k $ мы видим изменение эффективности, без модификации алгоритма или метрики. В итоге, график ниже показывает, что рукописные данные вполне согласуются с аксиомой ближайших соседей.</p>

<p><img src="http://flipback.github.com/images/posts/k-vs-percentage-correct.png" title="A graph of classification accuracy against k for values of k between 1 and 50. The graph clearly shows a downward trend as k increases, but all values k &lt; 10 are comparably go" alt="" /></p>

<p>Также возможно улучшить эту программу и мы могли бы сделать более подходящий алгоритм. Но рассматривая, что алгоритм использует необработанные данные и не манипулирует с с ними, результат не так уж плох.</p>

<p>В качестве примечания, могло бы быть гораздо интереснее, взять какое-нибудь планшетное программное обеспечение и использовать данный метод для распознавания цифр написанных на нем. Но увы, у нас мало времени для такого рода приложений.</p>

<h2>Преимущество, улучшения и проблемы.</h2>

<p>Первое преимущество <em>k-ближайших соседей</em> - общий и широко-известный алгоритм, который легко реализуется. Конечно, мы сделали ядро алгоритма всего в три строчки. Но данный алгоритм легко распараллелить и он от природы гибок. В отличии от <a href="http://jeremykun.wordpress.com/2011/08/11/the-perceptron-and-all-the-things-it-cant-perceive/">персептрона</a>, который полагается на линейное разделение, метод соседей и аксиома соседства доступны для данных, которые представлены в виде разных геометрических фигур. В этих <a href="http://courses.cs.tamu.edu/rgutier/cs790_w02/l8.pdf">лекциях</a> есть хорошие примеры, как показано ниже, и читатель может конечно сделать еще больше фокусов.</p>

<p><img src="http://flipback.github.com/images/posts/concentric-circles-knn.png" title="k-nearest-neighbors applied to a data set organized in concentric circles." alt="" /></p>

<p>Конечно, гибкость еще больше, в силу того, что можно использовать любые метрики для подсчета дистанций. Можно для примера, использовать <a href="http://www.improvedoutcomes.com/docs/WebSiteDocs/Clustering/Clustering_Parameters/Manhattan_Distance_Metric.htm">метрику Манхэттена</a>, если точки располагаются в городе. Возможности ограничиваются только нахождением новых и полезных метрик.</p>

<p>С такой популярностью KБС часто используется с некоторыми модификациями и улучшениями. Одно из улучшений - это гистерезис, который удаляет определенные точки рядом с границей решения. Такая техника называется модифицированный алгоритм КБС. Другое улучшение - это утяжелять определенные свойства при вычислении дистанции, которые требуют программного определения, какое свойство меньше полезно при классификации. Это уже ближе к области деревьев решений, поэтому мы оставим все это для тренировки читателя.</p>

<p>Следующее улучшение должно оптимизировать время выполнения. Если дано $ n $ учебных точек и $ d $ свойств, одна точка требует $ O(nd) $ для классификации. Это особенно расточительно, потому что большинство вычислений дистанций исполняются между точками, которые очень сильно рассеяны, и как $ k $ обычно мал, они не влияют на классификацию.</p>

<p>Один способ облегчить - это хранить точки в структуре, так называемых <a href="http://ru.wikipedia.org/wiki/K-%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D0%B5_%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE">k-деревья</a>. Данный метод возник в вычислительной геометрии в задаче о положении точки. Он разделяет пространство на части, основанные на количестве точек в каждом участке, и организовывает их в дерево. Другими словами, он может выделить узкую область, где точки расположены плотно, и грубо, где их меньше. Каждый шаг обхода дерева, может проверить, в какой суб-области лежат неклассифицириумые  точки, и решить надлежащим образом. С определенной гарантией, это уменьшит вычисления до $ O(log(n)d) $. К несчастью, есть проблемы в случае большого количества измерений, что выходит за рамки этого поста. Мы планируем исследовать k-d деревья в будущей серии о вычислительной геометрии.</p>

<p>Последняя проблема, которую мы рассмотрим - масштабирование данных. А именно, нужно быть осторожным, когда конвертируем реальные данные в числа. Мы можем думать о каждом свойстве, как о случайной переменной, и мы хотим, что бы все эти переменные имели сопоставимые изменения. Причина проста - мы используем <em>сферы</em>. Можно описать КБС, как поиск наименьшей сферы с центром в безымянной точке, которая содержит $ k $  именных точек данных, и использовать большее общее этих точек, что бы классифицировать новую. Конечно, можно говорить &ldquo;сфера&rdquo; в любом метрическом пространстве; это просто множество точек на фиксированном расстоянии от центра. Важное замечание то, что сфера имеет одинаковую длину для все осей. Если данные масштабируются не правильно, когда геометрия сферы не отражает геометрию данных, и алгоритм будет спотыкаться.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NMatrix -a Numeric Library for Mathematicians in Ruby]]></title>
    <link href="http://flipback.github.com/blog/2012/08/13/nmatrix-a-numeric-library-for-mathematicians-in-ruby/"/>
    <updated>2012-08-13T08:37:00+06:00</updated>
    <id>http://flipback.github.com/blog/2012/08/13/nmatrix-a-numeric-library-for-mathematicians-in-ruby</id>
    <content type="html"><![CDATA[<p>A few words about very interesting project - <a href="https://github.com/SciRuby/nmatrix">NMatrix</a>. At the moment Python is the best chose of dynamic languages to work with math. It has many very stable and quality libraries such as <a href="http://numpy.scipy.org/">NumPy</a>, <a href="http://www.scipy.org/">SciPy</a>, <a href="http://matplotlib.sourceforge.net/">matlib</a> and etc. But all may change&hellip;</p>

<!-- more -->


<p>The <a href="http://sciruby.com/">SciRuby</a> a project has goal to give powerful math instruments for Ruby programmers. It consists of several libraries for calculation and visualisation. The NMatrix is core library of the SciRuby for linear algebra written in C and C++. It has architecture that likes to <a href="http://narray.rubyforge.org/">NArray</a> and aims to maximum performance. Currently the library have status pre-alpha and is reborning to more clear code with C++ templates. But a new release will come very soon (this month).</p>

<p>I&rsquo;m contributing the project and working around slicing operation. It&rsquo;s very interesting for me and I feel I do something important. Because NMatrix has three types(<strong>dense</strong>, <strong>list</strong>, <strong>yale</strong>) to storage elements I need to implement three different algorithms (currently I have done a <strong>dense</strong> type). I hope it maybe interesting for readers and I want to write tree articles and try to describe structures of data and algorithms.</p>

<p>But while, you can play (for install <a href="https://github.com/SciRuby/nmatrix/wiki/NMatrix-Installation">see</a>) :</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="nb">require</span> <span class="s1">&#39;nmatrix&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="n">q</span> <span class="o">=</span> <span class="no">NMatrix</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="o">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="o">]</span><span class="p">,</span> <span class="o">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="o">]</span><span class="p">)</span>
</span><span class='line'><span class="n">q</span><span class="o">.</span><span class="n">pretty_print</span>
</span></code></pre></td></tr></table></div></figure>


<p>For more information see <a href="https://github.com/SciRuby/nmatrix/wiki/NMatrix">wiki</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Перевод 'Machine Learning. Introduction.']]></title>
    <link href="http://flipback.github.com/blog/2012/08/12/translation-of-machine-learning-introduction-dot/"/>
    <updated>2012-08-12T15:58:00+06:00</updated>
    <id>http://flipback.github.com/blog/2012/08/12/translation-of-machine-learning-introduction-dot</id>
    <content type="html"><![CDATA[<h2>От переводчика(меня).</h2>

<p>Не так давно наткнулся на очень интересный ресурс <a href="http://jeremykun.wordpress.com/">Math&amp;Programming</a>, который, как вы наверно догадались по названию, посвящен математике и программированию. И поскольку, мне самому очень интересна тема искусственного интеллекта, решил начать перевод серии постов посвященной машинному обучению. Таким образом, я надеюсь убить 2-х зайцев: разобраться в материале самому и подтянуть навыки технического перевода. Ну конечно, я очень надеюсь, что это будет еще кому-то полезно.</p>

<!-- more -->


<h1>Машинное обучение. Введение.</h1>

<p>Источник <a href="http://jeremykun.wordpress.com/2012/08/04/machine-learning-introduction/">Machine Learning — Introduction</a></p>

<h2>Серия постов о машинном обучении.</h2>

<p>В наши дни существует ошеломляющее количество исследований и разработок в очень грубо определенной научной области под названием машинное обучение. Отчасти, она так грубо определена потому, что она занимает методы из многих других областей науки. Множество задач в машинном обучении могут быть сформулированы разными, но эквивалентными способами. Пока они являются обычной задачей оптимизации, такие методы могут быть выражены в терминах статистических условий, иметь биологическую интерпретацию или иметь отчетливо геометрический или топологический вид. Как результат, машинное обучение (далее МО) стало пониматься как набор методов, т.е. противоположно единой теории.</p>

<p>Неудивительно, почему такое количество математики поддерживает эту диверсионную дисциплину. Практики (т.е. создатели алгоритмов) опираются  на статистику, линейную алгебру, выпуклую оптимизацию и интересуются теорией графов, функциональным анализом и топологией. Конечно, так или иначе, машинное обучение фокусируется на алгоритмах и данных.</p>

<p>Основной шаблон, который мы будем видеть снова и снова, когда мы будем выводить и реализовывать различные методы, это разработка алгоритма или математической модели, тестирование результата на наборе данных, и улучшение модели, основанной на предметно-ориентированных знаниях. Первый шаг обычно включает в себя &ldquo;прыжок веры&rdquo;, основанный на некой математической интуиции. Второй шаг - горсть надежных и хорошо понятых входных данных (часто взятых из <a href="http://archive.ics.uci.edu/ml/">базы данных Университета Калифорнии или Ирвина</a> и существует некоторый спор какая из практик повсеместна). Третий шаг часто является настройкой &ldquo;с бубном&rdquo; алгоритма и входных данных, чтобы дополнить одно другим.</p>

<p>Автор убежден, что математический фундамент - это самая важная часть МО, которая отражается в эффективности деталей реализации алгоритма. Цель МО - это представить данные, взятые из реального мира и имеющие свойственную реальным объектам структуру, и использовать их; дать настоящий результат - нечто должно абстрактно анализировать и представлять данные. Таким образом, этот блог будет фокусироваться в основном на математику, лежащую в основе алгоритмов и структур данных.</p>

<h2>Общий план</h2>

<p>Пока мы намереваемся покрыть классический набор тем МО, такие как нейронные сети и дерева решений, мы хотели бы бегло показать более сложные современные методы, такие как метод опорных векторов (<em>suppot vector machines</em>) и методы основанные на Колмогоровской сложности. Итак, мы публикуем ниже честолюбивый список тем (в необязательном порядке):</p>

<ol>
<li> <a href="http://flipback.github.com/blog/2012/08/27/k-nearest-neighbors-and-handwritten-digit-classification/">К-ближайших соседей</a></li>
<li> Дерево решений</li>
<li> Кластеризация по центру и по плотности</li>
<li> Нейронные сети</li>
<li> Метод опорных векторов</li>
<li> Регрессия</li>
<li> Байесовый вывод и сети</li>
<li> Методы основанные на сложности Колмогорова</li>
<li> Гомологии обучения и представления.</li>
</ol>


<p>Этот длинное и окольное путешествие будет неизбежно требовать произвольно большой(но конечной!) математической базы. Мы будем касаться: метрического пространства, математического анализа, теории вероятности, абстрактной алгебры, топологии и даже теории категорий. Замете, что некоторые эзотерические (т.е. продвинутые) темы будут иметь также свою собственную серию.</p>

<p>Конечно, как мы отмечали прежде, пока математики движимы желанием формализовывать  идеи,  программисты  мотивируются тем, что они могут сделать. Итак, мы заинтересованны использовать МО для выполнения крутых задач. Часть идей мы планируем реализовать в этом блоге: анализ социальных сетей, машинное зрение и распознавание текста, определение спама, обработка естественной речи, и классификация содержания и рекомендации.</p>

<p>В конце концов, нас интересует теоретическая граница того, чему можно научить компьютер. В стороне от практического использования, эта область изучения может потребовать от нас строго определения -что же значит &ldquo;учить&rdquo; машину. Эта область известна как <a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%A2%D0%B5%D0%BE%D1%80%D0%B8%D1%8F_%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F">теория вычислительного обучения</a>, и полезно для этого будет посвятить типичные сложнотеоретические вопросы, типа: &ldquo;Может ли это класс функций классификации стать обученным за <a href="http://ru.wikipedia.org/wiki/%D0%9A%D0%BB%D0%B0%D1%81%D1%81_P">полиномиальное время</a>?&rdquo;. Дополнительно, это включает изучение теорий больше из области статистики, например &ldquo;Вероятно-приближенная коррекция&rdquo; модели. Мы планируем изучить каждую из таких моделей и подключить их к нашим исследованиям, когда они потребуются.</p>

<p>Пока все.</p>
]]></content>
  </entry>
  
</feed>
